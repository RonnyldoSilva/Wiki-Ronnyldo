# Vision Transformer (ViT) â€” Como funciona e Casos de Uso

## ğŸ§  Como o ViT funciona â€” VisÃ£o geral rÃ¡pida

| Etapa | O que acontece | Porâ€¯quÃª isso importa |
|-------|----------------|----------------------|
| **1. Fatiar a imagem em â€œpatchesâ€** | A imagem \(H \times W\) Ã© dividida em blocos fixos (ex. 16â€¯Ã—â€¯16â€¯px). Cada patch Ã© achatado em um vetor. | Permite tratar a imagem como uma â€œsequÃªnciaâ€ â€” o mesmo formato que o Transformer usa para palavras. |
| **2. ProjeÃ§Ã£o linear** | Cada vetorâ€‘patch passa por uma camada linear (peso compartilhado) que o leva a um espaÃ§o de dimensÃ£o \(D\) (ex. 768). | Equivale ao *embedding* de palavras em NLP. |
| **3. AdiÃ§Ã£o dos embeddings de posiÃ§Ã£o** | Um vetor fixo ou aprendido Ã© somado a cada patchâ€‘token. | Transforma a sequÃªncia em algo â€œordenadoâ€ â€” o modelo sabe qual patch veio de onde. |
| **4. Token especial `[CLS]`** | Um token extra Ã© inserido Ã  frente da sequÃªncia. Ele vai concentrar a â€œvisÃ£o geralâ€ da imagem. | Depois do encoder, o vetor `[CLS]` vira a representaÃ§Ã£o global para classificaÃ§Ã£o ou outras tarefas. |
| **5. Encoder Transformer** | Camadas empilhadas de **Multiâ€‘Head Selfâ€‘Attentionâ€¯+â€¯MLP** com *LayerNorm* e *Dropout*. | Assim como no BERT, cada token presta atenÃ§Ã£o em todos os outros, mas aqui os â€œpalavrasâ€ sÃ£o patches de imagem. |
| **6. CabeÃ§a de saÃ­da** | â€‘ **ClassificaÃ§Ã£o**: MLP sobre `[CLS]`.  <br>â€‘ **DetecÃ§Ã£o / SegmentaÃ§Ã£o**: heads extras (ex. DETR, Mask2Former). | O mesmo backbone ViT serve para vÃ¡rias tarefas â€” muda sÃ³ a cabeÃ§a. |

> **IntuiÃ§Ã£oâ€‘chave:** Ao contrÃ¡rio dos CNNs, o ViT nÃ£o usa convoluÃ§Ãµes locais: ele aprende relaÃ§Ãµes **globais** logo no comeÃ§o. Isso o torna muito flexÃ­vel, mas exige dados (ou prÃ©â€‘treino) para generalizar bem.

---

## âš–ï¸ Pontos fortes vs fracos

| Pontos fortes | Pontos fracos |
|---------------|---------------|
| Capta dependÃªncias **longas** desde a 1Âª camada. | Selfâ€‘attention quadrÃ¡tica â†’ alto custo computacional. |
| **Transfer learning** simples: basta trocar a cabeÃ§a. | Precisa de **muito dado** (ou prÃ©â€‘treino em JFT/Imagenetâ€‘21k). |
| Arquitetura unificada com NLP â†’ fÃ¡cil para multimodal. | Menos interpretÃ¡vel que filtros de CNN. |

---

## ğŸš€ Casos de uso onde ViT brilha

| Categoria | Exemplos prÃ¡ticos |
|-----------|------------------|
| **ClassificaÃ§Ã£o de imagem** | InspeÃ§Ã£o de defeitos, raioâ€‘x, espÃ©cies animais, etc. |
| **DetecÃ§Ã£o / SegmentaÃ§Ã£o** | Usado em DETR, Mask2Former, Segmenter. |
| **VisÃ£o + Texto (Multimodal)** | CLIP, BLIP, etc. â€” busca de imagens por texto. |
| **VÃ­deo** | TimeSformer, ViViT â€” detecÃ§Ã£o de aÃ§Ãµes. |
| **Self-supervised Learning** | MAE, DINO, SimMIM â€” ideal para domÃ­nios com pouco rÃ³tulo. |
| **GeraÃ§Ã£o / DifusÃ£o** | Stable Diffusion, Uâ€‘ViT. |
| **CiÃªncia / Pesquisa** | Microscopia, imagens mÃ©dicas, astronomia, etc. |

---

## âŒ Quando **nÃ£o** usar

- Poucos dados + pouca GPU â†’ CNNs prÃ©â€‘treinadas performam melhor.
- AplicaÃ§Ãµes em tempo real com RAM limitada.
- Tarefas onde o detalhe local Ã© mais importante que o contexto global.

---

## âœ… Boas prÃ¡ticas de uso

1. Use pesos **prÃ©â€‘treinados** (`timm`, `huggingface`).
2. Aumente resoluÃ§Ã£o (ex: 224px â†’ 384px) no fineâ€‘tuning.
3. RegularizaÃ§Ã£o com `DropPath`, `WeightDecay`.
4. Use **data augmentation pesado**: RandAugment, CutMix, Mixup.
5. Se faltar memÃ³ria, opte por **variantes leves**: ViTâ€‘tiny, Swin, DeiT.

---

## TL;DR

Vision Transformers dividem imagens em **patches**, processam como sequÃªncia via **self-attention** e entendem contextos **globais** rapidamente. SÃ£o Ã³timos para classificaÃ§Ã£o, detecÃ§Ã£o, segmentaÃ§Ã£o e multimodalidade â€” desde que haja dados e hardware. Para cenÃ¡rios leves ou muito locais, CNNs ainda reinam.
